# 关键词提取技术总结

## 概述
关键词：能够表达文档中心内容的词语，常用于计算机系统标引论文内容特征、信息检索、系统汇集以供读者检阅。
**关键词提取是文本挖掘领域的一个分支**，也是**文本检索**、文档比较、摘要生成、文档**分类和聚类**等**文本挖掘研究**的基础性工作。

从算法的角度来看，关键词提取算法主要有两类：**无监督关键词提取方法**和**有监督关键词提取方法**。
##无监督关键词提取方法
**无需人工标注的语料，而是利用某些方法发现文本中比较重要的词作为关键词**，进行关键词提取——先抽取出候选词，然后对各个候选词进行打分，然后输出**topK个分值最高的候选词作为关键词**。根据打分的策略不同，有不同的算法，例如TF-IDF，TextRank，LDA等算法。
无监督关键词提取方法主要有三类：
- 基于统计特征的关键词提取算法，如TF，TF-IDF，其思想是利用文档中词语的统计信息抽取文档的关键词；
- 基于词图模型的关键词提取，如PageRank,TextRank。首先要构建文档的语言网络图，然后对语言进行网络图分析，在这个图上寻找具有重要作用的词或者短语，这些短语就是文档的关键词；
- 基于主题关键词提取算法，如LDA主要利用主题模型中关于主题分布的性质进行关键词提取。

##有监督关键词提取方法
将关键词抽取过程视为**二分类问题**，先提取出候选词，然后对于**每个候选词划定标签，要么是关键词，要么不是关键词**，然后训练关键词抽取分类器。
当新来一篇文档时，提取出所有的候选词，然后利用训练好的关键词提取分类器，对各个候选词进行分类，最终将标签为关键词的候选词作为关键词。

##两种方法比较
无监督方法不需要人工标注训练集合的过程，因此更加快捷，但由于无法有效综合利用多种信息 对候选关键词排序，所以效果无法与有监督方法媲美；
而有监督方法可以通过训练学习调节多种信息对于判断关键词的影响程度，因此效果更优，
**有监督的文本关键词提取算法需要高昂的人工成本**，因此现有的文本关键词提取**主要采用适用性较强的无监督关键词提取**。

##常用的关键词提取工具
jieba，[Textrank4zh](https://pypi.org/project/textrank4zh/0.3/)——TextRank算法工具，[SnowNLP](https://pypi.org/project/snownlp/)——（中文分析）简体中文文本处理，[TextBlob](https://pypi.org/project/textblob/)——英文分析

## LDA主题模型关键词提取算法及实现
LDA（Latent Dirichlet Allocation），是一个文档主题生成模型。
（事实上在机器学习领域，LDA是两个常用模型的简称：Linear Discriminant Analysis 和 **Latent Dirichlet Allocation**。）我们这里的LDA仅指代Latent Dirichlet Allocation，其在主题模型中占有非常重要的地位，常用来文本分类。
要了解LDA，我们首先就要来了解[什么是主题模型](https://www.jiqizhixin.com/graph/technologies/e49b21d8-935a-4da6-910d-504c79b9785f)，这里引用[机器之心网站](www.jiqizhixin.com)的这篇介绍：
>主题模型（Topic Model），机器学习和自然语言处理等领域是用来**在一系列文档中发现抽象主题**的一种统计模型——直观来讲，如果一篇文章有一个中心思想，那么一些特定词语会更频繁的出现。 比方说，如果一篇文章是在讲狗的，那“狗”和“骨头”等词出现的频率会高些。
>但实际上，一篇文章通常包含多种主题，而且每个主题所占比例各不相同。因此，如果一篇文章 10% 和猫有关，90% 和狗有关，那么和狗相关的关键字出现的次数大概会是和猫相关的关键字出现次数的 9 倍。
一个主题模型试图用数学框架来体现文档的这种特点。
>目前最流行的技术主要有四种：潜在语义分析（LSA）、概率潜在语义分析（pLSA）、潜在狄利克雷分布（LDA），以及最新的、基于深度学习的 lda2vec。但所有主题模型都基于相同的基本假设：
每个文档包含多个主题；而每个主题又包含多个单词。
>换句话说，主题模型围绕着以下观点构建——文档的语义由一些我们所忽视的隐变量或「潜」变量管理。因此，主题建模的目标就是揭示这些潜在变量——也就是主题，正是它们塑造了我们文档和语料库的含义。主题模型自动分析每个文档，统计文档内的词语，根据统计的信息来断定当前文档含有哪些主题，以及每个主题所占的比例各为多少。主题模型最初是运用于自然语言处理相关方向
>主题建模是一种常用的**文本挖掘工具，用于在文本体中发现隐藏的语义结构**。

LDA也称三层贝叶斯概率模型，这是因为其包含词、主题和文档三层结构。利用文档中单词的共现关系来对单词按主题聚类，得到“文档-主题”和“主题-单词”2个概率分布。

## Word2Vec 词向量表示
利用浅层神经网络模型自动学习词语在语料库中的出现情况，把词语嵌入到一个高维的空间中，通常在100-500维，
在高维空间中词语被表示为词向量的形式。**特征词向量的抽取是基于已经训练好的词向量模型**。

**K-means聚类算法**
**聚类算法**旨在**数据中发现数据对象之间的关系，将数据进行分组**，使得**组内的相似性尽可能的大**，**组间的相似性尽可能的小**。
算法思想：首先随机选择K个点作为初始质心，K为用户指定的所期望的簇的个数，通过计算每个点到各个质心的距离，将每个点指派到最近的质心形成K个簇，然后根据指派到簇的点重新计算每个簇的质心，重复指派和更新质心的操作，直到簇不发生变化或达到最大的迭代次数则停止。


**基于Word2Vec词聚类关键词提取方法的实现过程**
主要思路：对于用词向量表示的词语，通过K-Means算法对文章中的词进行聚类，选择聚类中心作为文本的一个主要关键词，
计算其他词与聚类中心的距离即相似度，选择topK个距离聚类中心最近的词作为关键词，而这个词间相似度可用Word2Vec生成的向量计算得到。
具体步骤如下：
1. 对语料进行Word2Vec模型训练，得到词向量文件；
2. 对文本进行预处理获得N个候选关键词；
3. 遍历候选关键词，从词向量文件中提取候选关键词的词向量表示；
4. 对候选关键词进行K-Means聚类，得到各个类别的聚类中心（需要人为给定聚类的个数）；
5. 计算各类别下，组内词语与聚类中心的距离（欧几里得距离或曼哈顿距离），按聚类大小进行降序排序；
6. 对候选关键词计算结果得到排名前TopK个词语作为文本关键词。

附注：第三方工具包Scikit-learn提供了K-Means聚类算法的相关函数



## 互信息关键词提取算法及实现

**互信息（Mutual Information，MI）**：概率论和信息论中，两个随机变量的互信息或转移信息（transinformation）是变量间相互依赖性的量度。



特点：不同于相关系数，互信息并不局限于实值随机变量，它更加一般且决定着联合分布 $p(X,Y)$ 和分解的边缘分布的乘积 $p(X)p(Y)$ 的相似程度。互信息是度量两个事件集合之间的相关性(mutual dependence)。



作用：互信息被广泛用于度量一些语言现象的相关性。在信息论中，互信息常被用来**衡量两个词的相关度**，也用来计算**词与类别之间的相关性**。



计算公式如下，其中$p(x,y)$是X和Y的联合概率分布函数，$p(x)$和$p(y)$分别为X和Y的边缘概率分布函数。

![互信息计算公式](![img](https://img-blog.csdnimg.cn/2019072320171787.png)

**信息论中的互信息和决策树中的信息增益的关系** 

下面两条公式表达意思是一样的——**得知特征X的信息而使得类Y的信息的不确定性减少的程度**：

- 信息论中互信息：$I(X;Y)=H(X)-H(X|Y)$
- 决策树中信息增益：$G(D,A)=H(D)-H(D|A)$

**标准化互信息（Normalized Mutual Information，NMI）**，用来衡量两种聚类结果的相似度。
标准化互信息的`Sklearn`实现：`metrics.normalized_mutual_info_score(y_train, x_train[:, i])`。
**点互信息（`Pointwise Mutual Information，PMI`）**，该指标用于衡量两个事物之间的相关性（比如两个词）。

## 卡方检验(Chi-square test/Chi-Square Goodness-of-Fit Test)

定义：数理统计中用于**检验两个变量独立性的方法**，是一种**确定两个分类变量之间是否存在相关性**的统计方法，经典的卡方检验是**可以检验定性自变量对定性因变量的相关性**。基本思路

- 原假设：两个变量是独立的
- 计算实际观察值和理论值之间的偏离程度
- 如果偏差足够小，小于设定阈值，就接受原假设；否则就否定原假设，认为两变量是相关的。

计算公式如下，其中，A为实际值，T为理论值。

![计算公式](https://img-blog.csdnimg.cn/20190723204129786.png)

**卡方检验在`NLP`领域的应用：卡方检验可用于文本分类问题中的特征选择，此时不需要设定阈值，只关心找到最为相关的`topK`个特征。基本思想：比较理论频数和实际频数的吻合程度或者拟合优度问题。**



## 基于树模型的关键词提取算法及实现

所使用的树模型主要包括决策树和随机森林，基于树的预测模型（sklearn.tree 模块和 sklearn.ensemble 模块）能够用来计算特征的重要程度，因此能用来去除不相关的特征（结合 sklearn.feature_selection.SelectFromModel）

sklearn.ensemble模块包含了两种基于随机决策树的平均算法：RandomForest算法和Extra-Trees算法。这两种算法都采用了很流行的树设计思想：perturb-and-combine思想。这种方法会在分类器的构建时，通过引入随机化，创建一组各不一样的分类器。这种ensemble方法的预测会给出各个分类器预测的平均。

``RandomForests `在随机森林（RF）中，该ensemble方法中的每棵树都基于一个通过可放回抽样（boostrap）得到的训练集构建。另外，在构建树的过程中，当split一个节点时，split的选择不再是对所有features的最佳选择。相反的，在features的子集中随机进行split反倒是最好的split方式。sklearn的随机森林（RF）实现通过对各分类结果预测求平均得到，而非让每个分类器进行投票（vote）。
Ext-Trees 在Ext-Trees中(详见ExtraTreesClassifier和 ExtraTreesRegressor)，在该方法中，**随机性在划分时会更进一步进行计算**。在随机森林中，会使用侯选feature的一个随机子集，而非查找最好的阈值，对于每个候选feature来说，阈值是抽取的，选择这种随机生成阈值的方式作为划分原则。

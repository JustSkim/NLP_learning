{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 英文文本分类——电影评论情感判别\n",
    "博客链接地址：https://asialee.blog.csdn.net/article/details/86497085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\NLP学习路径总结\\data\\english_data\\labeledTrainData.tsv\n",
      "Number of reviews:25000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\"The Classic War of the Worlds\" by Timothy Hin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \"The Classic War of the Worlds\" by Timothy Hin...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#用pandas读入训练数据\n",
    "#datafile=os.path.join('f:\\\\NLP学习路径总结\\data\\english_data','labeledTrainData.tsv')\n",
    "datafile='E:\\\\NLP学习路径总结\\\\data\\\\english_data\\\\labeledTrainData.tsv'\n",
    "print(datafile)\n",
    "df=pd.read_csv(\"labeledTrainData.tsv\",sep='\\t',escapechar='\\\\')\n",
    "print('Number of reviews:{}'.format(len(df)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 构建停用词列表数据\n",
    "#words_nostop=[w for w in words if w not in stopwords.words('english')]\n",
    "stopwords={}.fromkeys([line.rstrip() for line in open('F:\\\\NLP学习路径总结\\\\data\\\\english_data\\\\stopwords.txt')])\n",
    "eng_stopwords=set(stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来要对数据做预处理，分为以下几个步骤：\n",
    "1. 去掉html标签\n",
    "1. 移除标点符号\n",
    "1. 将句子切分成词\n",
    "1. 去掉停用词\n",
    "1. 重组为新的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text=BeautifulSoup(text,'html.parser').get_text()\n",
    "    text=re.sub('[^a-zA-Z]',' ',text)\n",
    "    words=text.lower().split()\n",
    "    words=[w for w in words if w not in eng_stopwords]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda3\\envs\\py37\\lib\\site-packages\\bs4\\__init__.py:439: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "      <td>stuff moment mj ve started listening music wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\"The Classic War of the Worlds\" by Timothy Hin...</td>\n",
       "      <td>classic war worlds timothy hines entertaining ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "      <td>film starts manager nicholas bell giving inves...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "      <td>assumed praised film greatest filmed opera did...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "      <td>superbly trashy wondrously unpretentious explo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review  \\\n",
       "0  5814_8          1  With all this stuff going down at the moment w...   \n",
       "1  2381_9          1  \"The Classic War of the Worlds\" by Timothy Hin...   \n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...   \n",
       "3  3630_4          0  It must be assumed that those who praised this...   \n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8...   \n",
       "\n",
       "                                        clean_review  \n",
       "0  stuff moment mj ve started listening music wat...  \n",
       "1  classic war worlds timothy hines entertaining ...  \n",
       "2  film starts manager nicholas bell giving inves...  \n",
       "3  assumed praised film greatest filmed opera did...  \n",
       "4  superbly trashy wondrously unpretentious explo...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将清洗的数据放到dataframe中\n",
    "df['clean_review']=df.review.apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-7-bb5f642861fb>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[0msentences\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'first'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'sentence'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;34m'second'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'sentence'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 17\u001B[1;33m \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mWord2Vec\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msentences\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msentences\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mworkers\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnum_workers\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mvector_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnum_features\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmin_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmin_word_count\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwindow\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcontext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msample\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdownsampling\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     18\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;31m# If you don't plan to train the model any further, calling\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mG:\\anaconda3\\envs\\py37\\lib\\site-packages\\gensim\\models\\word2vec.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001B[0m\n\u001B[0;32m    428\u001B[0m                 \u001B[0mcorpus_iterable\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcorpus_iterable\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcorpus_file\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcorpus_file\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtotal_examples\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcorpus_count\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    429\u001B[0m                 \u001B[0mtotal_words\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcorpus_total_words\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstart_alpha\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0malpha\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 430\u001B[1;33m                 end_alpha=self.min_alpha, compute_loss=self.compute_loss, callbacks=callbacks)\n\u001B[0m\u001B[0;32m    431\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    432\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mtrim_rule\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mG:\\anaconda3\\envs\\py37\\lib\\site-packages\\gensim\\models\\word2vec.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m   1040\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mepochs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1041\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1042\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_check_training_sanity\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtotal_examples\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtotal_examples\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtotal_words\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtotal_words\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1043\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_check_corpus_sanity\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcorpus_iterable\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcorpus_iterable\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcorpus_file\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcorpus_file\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpasses\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1044\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mG:\\anaconda3\\envs\\py37\\lib\\site-packages\\gensim\\models\\word2vec.py\u001B[0m in \u001B[0;36m_check_training_sanity\u001B[1;34m(self, epochs, total_examples, total_words, **kwargs)\u001B[0m\n\u001B[0;32m   1538\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1539\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkey_to_index\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# should be set by `build_vocab`\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1540\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"you must first build vocabulary before training the model\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1541\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvectors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1542\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"you must initialize vectors before training the model\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "vectorizer=CountVectorizer(max_features=5000)\n",
    "train_data_features=vectorizer.fit_transform(df.clean_review).toarray()\n",
    "train_data_features.shape\n",
    "\n",
    " \n",
    "# 设定词向量训练的参数\n",
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 40   # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "\n",
    "model = Word2Vec(sentences=sentences, workers=num_workers,vector_size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    " \n",
    "# If you don't plan to train the model any further, calling\n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    " \n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model.save(os.path.join('..', 'models', model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#构建随机森林分类器并训练\n",
    "forest=RandomForestClassifier(n_estimators=100)\n",
    "forest=forest.fit(train_data_features,df.sentiment)\n",
    " \n",
    "#删除不用的占内容变量\n",
    "del df \n",
    "del train_data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#读取测试数据并进行预测\n",
    "datafile=os.path.join('F:\\\\NLP学习路径总结\\\\data\\\\english_data','testData.tsv')\n",
    "df=pd.read_csv(datafile,sep='\\t',escapechar='\\\\')\n",
    "print('Number of reviews:{}'.format(len(df)))\n",
    "df['clean_review']=df.review.apply(clean_text)\n",
    "df.head()\n",
    " \n",
    "test_data_features=vectorizer.transform(df.clean_review).toarray()\n",
    "test_data_features.shape\n",
    " \n",
    "result=forest.predict(test_data_features)\n",
    "output=pd.DataFrame({'id':df.id,'sentiment':result})\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 最后一步，将预测结果写入csv文件\n",
    "\n",
    "output.to_csv(os.path.join('F:\\\\NLP学习路径总结\\\\data\\\\english_data','Bag_of_Words_model.csv'),index=False)\n",
    " \n",
    " \n",
    "del df\n",
    "del test_data_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最后一步，将预测结果写入csv文件\n",
    "\n",
    "output.to_csv(os.path.join('F:\\\\NLP学习路径总结\\\\data\\\\english_data','Bag_of_Words_model.csv'),index=False)\n",
    " \n",
    " \n",
    "del df\n",
    "del test_data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text=BeautifulSoup(text,'html.parser').get_text()\n",
    "    text=re.sub('[^a-zA-Z]',' ',text)\n",
    "    words=text.lower().split()\n",
    "    words=[w for w in words if w not in eng_stopwords]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda3\\envs\\py37\\lib\\site-packages\\bs4\\__init__.py:439: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "      <td>stuff moment mj ve started listening music wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\"The Classic War of the Worlds\" by Timothy Hin...</td>\n",
       "      <td>classic war worlds timothy hines entertaining ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "      <td>film starts manager nicholas bell giving inves...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "      <td>assumed praised film greatest filmed opera did...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "      <td>superbly trashy wondrously unpretentious explo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review  \\\n",
       "0  5814_8          1  With all this stuff going down at the moment w...   \n",
       "1  2381_9          1  \"The Classic War of the Worlds\" by Timothy Hin...   \n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...   \n",
       "3  3630_4          0  It must be assumed that those who praised this...   \n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8...   \n",
       "\n",
       "                                        clean_review  \n",
       "0  stuff moment mj ve started listening music wat...  \n",
       "1  classic war worlds timothy hines entertaining ...  \n",
       "2  film starts manager nicholas bell giving inves...  \n",
       "3  assumed praised film greatest filmed opera did...  \n",
       "4  superbly trashy wondrously unpretentious explo...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将清洗的数据放到dataframe中\n",
    "df['clean_review']=df.review.apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-14-d4834a829bc0>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[0mdownsampling\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m1e-3\u001B[0m   \u001B[1;31m# Downsample setting for frequent words\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 15\u001B[1;33m model = Word2Vec(sentences, workers=num_workers, \\\n\u001B[0m\u001B[0;32m     16\u001B[0m             \u001B[0msize\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnum_features\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmin_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmin_word_count\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m             window = context, sample = downsampling)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "vectorizer=CountVectorizer(max_features=5000)\n",
    "train_data_features=vectorizer.fit_transform(df.clean_review).toarray()\n",
    "train_data_features.shape\n",
    "\n",
    " \n",
    "# 设定词向量训练的参数\n",
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 40   # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    " \n",
    "model = Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    " \n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    " \n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model.save(os.path.join('..', 'models', model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建随机森林分类器并训练\n",
    "forest=RandomForestClassifier(n_estimators=100)\n",
    "forest=forest.fit(train_data_features,df.sentiment)\n",
    " \n",
    "#删除不用的占内容变量\n",
    "del df \n",
    "del train_data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取测试数据并进行预测\n",
    "datafile=os.path.join('F:\\\\NLP学习路径总结\\\\data\\\\english_data','testData.tsv')\n",
    "df=pd.read_csv(datafile,sep='\\t',escapechar='\\\\')\n",
    "print('Number of reviews:{}'.format(len(df)))\n",
    "df['clean_review']=df.review.apply(clean_text)\n",
    "df.head()\n",
    " \n",
    "test_data_features=vectorizer.transform(df.clean_review).toarray()\n",
    "test_data_features.shape\n",
    " \n",
    "result=forest.predict(test_data_features)\n",
    "output=pd.DataFrame({'id':df.id,'sentiment':result})\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最后一步，将预测结果写入csv文件\n",
    "\n",
    "output.to_csv(os.path.join('F:\\\\NLP学习路径总结\\\\data\\\\english_data','Bag_of_Words_model.csv'),index=False)\n",
    " \n",
    " \n",
    "del df\n",
    "del test_data_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}